import re
import hashlib
from typing import List, Dict
from app.ml.Embed.chunker.postprocess import generic_post_process_chunks
MIN_LEN = 100

SECTION_TITLES = [
    "recipient",
    "applicant_info",
    "title",
    "body",
    "signature",
    "service_data",
    "metadata",
]

SECTION_TO_TYPE = {
    "recipient": "intro",
    "applicant_info": "applicant_info",
    "title": "title",
    "body": "body",
    "signature": "signature",
    "service_data": "receipt",
    "metadata": "metadata"
}


SECTION_PATTERNS = {
    "recipient": r"^—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—é\s+(–ø–æ\s+–æ—Å–æ–±–æ\s+–≤–∞–∂–Ω—ã–º\s+–¥–µ–ª–∞–º\s+)?—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ\s+—É–ø—Ä–∞–≤–ª–µ–Ω–∏—è[^\n]{0,100}",
    "applicant_info": r"(–≥\.—Ä\.|–ø—Ä–æ–∂–∏–≤–∞—é—â(–∞—è|–∏–π)|[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å]\.[–ê-–Ø–Å]\.)",
    "title": r"\b–∑–∞—è–≤–ª–µ–Ω–∏–µ\b",
    "body": r"^–æ—Ç\s+–æ–∑–Ω–∞–∫–æ–º–ª–µ–Ω–∏—è.*|^(–æ–±|—è)\s+[^\n]{5,}",
    "signature": r"^[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å]\.[–ê-–Ø–Å]\.\s*\n\d{2}\.\d{2}\.\d{4}\s*–≥\.?",
    "service_data": r"^–∑–∞—è–≤–ª–µ–Ω–∏–µ\s+–ø—Ä–∏–Ω—è–ª[:]?.*",
    "metadata": r"(qr-–∫–æ–¥ —Å–æ–¥–µ—Ä–∂–∏—Ç|—ç—Ü–ø|mailto:|–ø–æ–¥–ø–∏—Å–∞–ª:|–µ–¥–∏–Ω—ã–π\s+—Ä–µ–µ—Å—Ç—Ä|–∏—Å\s+¬´–µ–¥–∏–Ω—ã–π\s+—Ä–µ–µ—Å—Ç—Ä)"
}


def normalize_text(text: str) -> str:
    text = text.lower()
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"[^\w\s.,:;()\[\]@-]", "", text)
    return text.strip()


def _chunk_by_sections(
    text: str,
    sections: Dict[str, str],
    filetype: str,
    document_id: int,
    case_id: int,
    user_id: int
) -> List[Dict]:
    chunks = []
    section_map = {}
    group_map = {}

    # —Å–æ–∑–¥–∞—ë–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∏–º–µ–Ω–∞ –≥—Ä—É–ø–ø
    for idx, (title, regex) in enumerate(sections.items()):
        group = f"SECTION_{idx}"
        section_map[group] = title
        group_map[title] = group

    try:
        pattern = re.compile(
            "|".join(f"(?P<{group}>{regex})" for group, regex in zip(group_map.values(), sections.values())),
            re.MULTILINE | re.IGNORECASE
        )
    except re.error as e:
        print(f"[regex compile error]: {e}")
        return []

    matches = list(pattern.finditer(text))

    for i, match in enumerate(matches):
        group = match.lastgroup
        title = section_map.get(group, "unknown")
        start = match.start()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
        chunk_text = text[start:end].strip()

        if len(chunk_text) < MIN_LEN and title not in ("signature", "metadata", "service_data"):
            continue

        chunk_type = SECTION_TO_TYPE.get(title, "other")
        chunk_subtype = title.lower().replace(" ", "_")

        chunks.append({
            "title": title.capitalize(),
            "chunk_type": chunk_type,
            "chunk_subtype": chunk_subtype,
            "text": chunk_text,
            "filetype": filetype,
            "case_id": case_id,
            "document_id": document_id,
            "user_id": user_id,
            "confidence": 1.0,
            "hash": hashlib.md5(chunk_text.encode("utf-8")).hexdigest(),
            "semantic_hash": hashlib.md5(normalize_text(chunk_text).encode("utf-8")).hexdigest(),
            "position": i + 1,
            "source_page": -1
        })

    return chunks


def post_process_chunks(chunks: List[Dict]) -> List[Dict]:
    """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤."""
    processed = []
    seen_hashes = set()
    position = 1

    for chunk in chunks:
        text = chunk.get("text", "").strip()
        if len(text) < MIN_LEN and chunk.get("chunk_type") not in ("signature", "metadata", "receipt"):
            continue

        chunk_hash = chunk.get("hash") or hashlib.md5(text.encode("utf-8")).hexdigest()
        if chunk_hash in seen_hashes:
            continue
        seen_hashes.add(chunk_hash)

        semantic_hash = chunk.get("semantic_hash") or hashlib.md5(normalize_text(text).encode("utf-8")).hexdigest()

        chunk.update({
            "hash": chunk_hash,
            "semantic_hash": semantic_hash,
            "confidence": 1.0,
            "position": position,
            "source_page": chunk.get("source_page", -1),
            "chunk_subtype": chunk.get("chunk_subtype", chunk.get("title", "").lower().replace(" ", "_"))
        })

        processed.append(chunk)
        position += 1

    # üîß –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –æ–±—â—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
    return generic_post_process_chunks(processed)


def chunk_zayavlenie(
    text: str,
    filetype: str,
    document_id: int,
    case_id: int,
    user_id: int
) -> List[Dict]:
    return _chunk_by_sections(text, SECTION_PATTERNS, filetype, document_id, case_id, user_id)
