import logging
import traceback
from app.core.weaviate_client import save_to_weaviate, client
from app.ml.Embed.smart_chunk import smart_chunk_document
from app.ml.Embed.embedder import get_embedding
from weaviate.classes.query import Filter
from app.ml.Embed.reranker import rerank_chunks

import hashlib
# üß† –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –æ–¥–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞)
GLOBAL_CHUNK_HASHES = set()

logger = logging.getLogger(__name__)

def hash_chunk(text: str) -> str:
    return hashlib.md5(text.strip().encode("utf-8")).hexdigest()

def index_full_document(
    title: str,
    text: str,
    filetype: str,
    user_id: int,
    case_id: int,
    document_id: int,
    doc_type: str
):
    try:
        chunks = smart_chunk_document(
            text=text,
            case_id=case_id,
            document_id=document_id,
            doc_type=doc_type
        )

        logger.info(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç '{title}' —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤ (–¥–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏).")

        for i, chunk in enumerate(chunks):
            chunk_text = chunk["text"]
            chunk_hash = hash_chunk(chunk_text)

            if chunk_hash in GLOBAL_CHUNK_HASHES:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω –≥–ª–æ–±–∞–ª—å–Ω—ã–π –¥—É–±–ª–∏–∫–∞—Ç —á–∞–Ω–∫–∞ {i + 1}")
                continue

            GLOBAL_CHUNK_HASHES.add(chunk_hash)

            vector = get_embedding(chunk_text)
            if not vector:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω –ø—É—Å—Ç–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —á–∞–Ω–∫–∞ {i + 1}")
                continue

            save_to_weaviate(
                title=f"{title}_chunk_{i + 1}",
                text=chunk_text,
                filetype=filetype,
                case_id=case_id,
                document_id=document_id,
                user_id=user_id,
                vector=vector
            )

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{title}': {str(e)}\n{traceback.format_exc()}")
        raise


def search_similar_chunks(query: str, case_id: int, k: int = 5) -> list[dict]:
    """
    –ü–æ–∏—Å–∫ –ø–æ Weaviate + reranking.
    """
    try:
        if not client.is_connected():
            logger.info("üîå –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Weaviate...")
            client.connect()

        question_vector = get_embedding(query)

        collection = client.collections.get("Document")
        result = collection.query.near_vector(
            near_vector=question_vector,
            filters=Filter.by_property("case_id").equal(case_id),
            limit=15
        )

        raw_results = result.objects
        if not raw_results:
            logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
            return []

        text_to_obj = {}
        for obj in raw_results:
            text = obj.properties.get("text")
            if text and text not in text_to_obj:
                text_to_obj[text] = obj.properties

        unique_texts = list(text_to_obj.keys())
        top_chunks = rerank_chunks(query, unique_texts, top_k=k)
        reranked_matches = [text_to_obj[text] for text in top_chunks if text in text_to_obj]

        logger.info(f"üîç –ü–æ—Å–ª–µ reranking –æ—Ç–æ–±—Ä–∞–Ω–æ {len(reranked_matches)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
        return reranked_matches

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —á–∞–Ω–∫–æ–≤: {str(e)}\n{traceback.format_exc()}")
        return []
